# -*- coding: utf-8 -*-
"""model_training_all2165_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dg_h40LXn9XdCLSXnGLkXmXfqhITb8Bi
"""

import pandas as pd
import numpy as np

df=pd.read_csv(r"features_final.csv")

df

df = df.drop(
    columns=[
    "Unnamed: 0","File Name","pseudocough","file_number"

    ],
    axis=1,
    errors='ignore'
)

df

df.info()

df = df.iloc[:-1]
df

def calculate_feature_target_correlation(dataframe, target_column):

    # Identify numeric columns (exclude target)
    numeric_columns = dataframe.select_dtypes(include=['float64', 'int64']).columns
    numeric_columns = [col for col in numeric_columns if col != target_column]

    # Calculate correlations
    correlations = {}

    # Pearson correlation for continuous variables
    pearson_corr = dataframe[numeric_columns + [target_column]].corr()[target_column][numeric_columns]

    # Spearman correlation for potential non-linear relationships
    spearman_corr = dataframe[numeric_columns + [target_column]].corr(method='spearman')[target_column][numeric_columns]

    # Store correlations
    correlations['Pearson'] = pearson_corr
    correlations['Spearman'] = spearman_corr

    return correlations

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,classification_report

def visualize_feature_correlations(correlations, title='Feature-Target Correlations'):
    # Prepare data for plotting
    pearson_corr = correlations['Pearson']
    spearman_corr = correlations['Spearman']

    # Create figure with two subplots
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

    # Pearson Correlation Plot
    pearson_corr.sort_values(ascending=False).plot(kind='bar', ax=ax1, color='blue', alpha=0.7)
    ax1.set_title('Pearson Correlation with Target Variable')
    ax1.set_xlabel('Features')
    ax1.set_ylabel('Correlation Coefficient')
    ax1.axhline(y=0, color='r', linestyle='--')

    # Spearman Correlation Plot
    spearman_corr.sort_values(ascending=False).plot(kind='bar', ax=ax2, color='green', alpha=0.7)
    ax2.set_title('Spearman Correlation with Target Variable')
    ax2.set_xlabel('Features')
    ax2.set_ylabel('Correlation Coefficient')
    ax2.axhline(y=0, color='r', linestyle='--')

    plt.tight_layout()
    plt.show()

    return fig

import seaborn as sns
def advanced_correlation_analysis(dataframe, target_column):

    # Calculate correlations
    correlations = calculate_feature_target_correlation(dataframe, target_column)

    # Visualize correlations
    visualize_feature_correlations(correlations)

    # Prepare comprehensive correlation report
    correlation_report = pd.DataFrame({
        'Pearson Correlation': correlations['Pearson'],
        'Spearman Correlation': correlations['Spearman'],
    })

    # Print top correlations
    print("\nFeatures by Correlation Strength:")
    print(correlation_report)


    return correlation_report

correlation_results = advanced_correlation_analysis(df, 'target')

# y=df['target']

# df=df.drop(columns=["target"], axis=1,errors='ignore')

# from sklearn.model_selection import train_test_split

# Xtrain, Xtest, ytrain, ytest = train_test_split(df, y,stratify=y,test_size=0.2)

# from imblearn.over_sampling import SMOTE
# import pandas as pd

# def diagnose_target_column(data, target_column='target'):
#     Target = data[target_column]
#     print(f"Target Column Diagnostic:\n{Target.value_counts()}")
#     return Target

# def apply_smote_balancing(data, target_column='target'):
#     # Create a copy to avoid modifying the original dataset
#     data = data.copy()

#     # Ensure the target column is discrete (int)
#     y = data[target_column].astype(int) if pd.api.types.is_numeric_dtype(data[target_column]) else pd.to_numeric(data[target_column], errors='coerce').fillna(0).astype(int)

#     # Separate features and target
#     X = data.drop(columns=[target_column])
#     print(f"Original class distribution:\n{y.value_counts()}")

#     # Apply SMOTE
#     smote = SMOTE(random_state=42)
#     X_resampled, y_resampled = smote.fit_resample(Xtrain, ytrain)

#     # Combine features and balanced target
#     balanced_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name=target_column)], axis=1)

#     print(f"Balanced class distribution:\n{pd.Series(y_resampled).value_counts()}")
#     return balanced_data

# # Example Usage
# # Diagnose target column
# diagnose_target_column(df, target_column='target')

# # Apply SMOTE balancing
# df= apply_smote_balancing(df, target_column='target')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

def diagnose_target_column(data, target_column='target'):
    """Diagnose the distribution of the target column."""
    target = data[target_column]
    print(f"Target Column Diagnostic:\n{target.value_counts(normalize=True) * 100}")
    print(f"Total samples: {len(target)}")
    return target

def apply_smote_balancing(X, y, test_size=0.2, random_state=42):

    # Initial train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, stratify=y, random_state=random_state
    )

    # Print original training distribution
    print("Original training class distribution:")
    print(y_train.value_counts(normalize=True) * 100)

    # Apply SMOTE to training data only
    smote = SMOTE(random_state=random_state)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

    # Print balanced training distribution
    print("\nBalanced training class distribution:")
    print(pd.Series(y_train_resampled).value_counts(normalize=True) * 100)

    return (
        X_train_resampled, X_test,
        y_train_resampled, y_test
    )

# Example usage
# Separate features and target
y = df['target']
X = df.drop(columns=['target'], axis=1, errors='ignore')

# Diagnose target column
diagnose_target_column(df)

# Apply SMOTE and train-test split
X_train, X_test, y_train, y_test = apply_smote_balancing(X, y)

# df.to_csv(r'C:\Users\OM\Desktop\Anvesshan\CSV_files\new_csv_modeified\new_features_balanced_withlabels.csv')

count_class_1 = (y_train== 1).sum()
count_class_2 = (y_train== 2).sum()
count_class_3 = (y_train == 3).sum()

print(f"Class 1 count: {count_class_1}")
print(f"Class 2 count: {count_class_2}")
print(f"Class 3 count: {count_class_3}")

df=df.iloc[:41].reset_index(drop=True)

df.to_csv(r'C:\Users\OM\Desktop\Anvesshan\CSV_files\new_csv_modeified\new_features_balanced.csv')

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# First, let's properly count the classes
# If y is a DataFrame
if isinstance(y, pd.DataFrame):
    count_class_1 = y_train[y_train.columns[0]].value_counts()[1]
    count_class_2 = y_train[y_train.columns[0]].value_counts()[2]
    count_class_3 = y_train[y_train.columns[0]].value_counts()[3]
# If y is a Series
elif isinstance(y, pd.Series):
    count_class_1 = y_train.value_counts()[1]
    count_class_2 = y_train.value_counts()[2]
    count_class_3 = y_train.value_counts()[3]

# Create the plot
classes = ['1', '2', '3']
counts = [count_class_1, count_class_2, count_class_3]

plt.figure(figsize=(10, 6))
bars = plt.bar(classes, counts)
plt.title('Distribution of Classes')
plt.xlabel('Classes')
plt.ylabel('Count')

# Add value labels on top of each bar
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{int(height)}',
             ha='center', va='bottom')

plt.show()

y_train.shape[0]



from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import accuracy_score
# Assuming Xtrain contains your training features and ytrain_discrete contains discrete class labels
new0=HistGradientBoostingClassifier()
new0.fit(X_train, y_train)

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform
param_distributions = {
    'learning_rate': uniform(loc=0.01, scale=0.2),
    'max_iter': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'min_samples_leaf': [1, 2, 4],
    'l2_regularization': uniform(loc=0.0, scale=1.0),
}
random_search = RandomizedSearchCV(new0, param_distributions, random_state=42)
random_search.fit(X_train, y_train)

best_estimator = random_search.best_estimator_
best_params = random_search.best_params_
best_params
best_estimator

new0=HistGradientBoostingClassifier(l2_regularization=0.9488855372533332,
                               learning_rate=0.20312640661491188, max_depth=5,
                               max_iter=200, min_samples_leaf=1)
new0.fit(X_train, y_train)

accuracy = round(new0.score(X_test, y_test)*100,2)
print(round(accuracy, 2), '%')

import joblib  # Import joblib for saving the model

# ... your existing code ...

# Save the model using joblib
joblib.dump(new0, "my_model.pkl")