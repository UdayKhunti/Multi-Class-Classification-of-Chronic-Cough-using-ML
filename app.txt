import numpy as np
import pandas as pd
import tensorflow as tf
from flask import Flask, request, jsonify, render_template

# Load the trained model (using h5 format instead of pkl)
model = tf.keras.models.load_model('my_model.h5')

# Load the scaler if needed for preprocessing
import joblib
scaler = joblib.load('scaler.joblib')
label_encoder = joblib.load('label_encoder.joblib')

app = Flask(__name__)

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # Check if the request is JSON or form data
        if request.is_json:
            data = request.get_json()
        else:
            data = request.form.to_dict()
        
        # Extract features from the request
        # List of expected features based on your training script
        expected_features = [
            'mean', 'max', 'min', 'std', 'kurtosis', 'power',
            'spectralentropy', 'rms', 'zcr', 'rolloff',
            'centroid', 'bandwidth', 'flatness', 'contrast',
            'ber', 'snr', 'THD', 'f0', 'mfcc_mean1', 'mfcc_mean2',
            'mfcc_mean3', 'mfcc_mean4', 'mfcc_mean5', 'mfcc_mean6',
            'mfcc_mean7', 'mfcc_mean8', 'mfcc_mean9', 'mfcc_mean10',
            'mfcc_mean11', 'mfcc_mean12', 'mfcc_mean13', 'mfcc_std1',
            'mfcc_std2', 'mfcc_std3', 'mfcc_std4', 'mfcc_std5',
            'mfcc_std6', 'mfcc_std7', 'mfcc_std8', 'mfcc_std9',
            'mfcc_std10', 'mfcc_std11', 'mfcc_std12', 'mfcc_std13'
        ]
        
        # Extract each feature from the request
        features = []
        for feature in expected_features:
            features.append(float(data.get(feature, 0)))
        
        # Convert to numpy array and reshape for prediction
        features_array = np.array(features).reshape(1, -1)
        
        # Apply scaling if needed
        features_scaled = scaler.transform(features_array)
        
        # Make prediction
        raw_prediction = model.predict(features_scaled)
        
        # For keras models, prediction might be probabilities
        # Get the class with highest probability
        if raw_prediction.shape[1] > 1:  # One-hot encoded output
            predicted_class_idx = np.argmax(raw_prediction, axis=1)[0]
            # If using label encoder, transform back to original label
            predicted_class = label_encoder.inverse_transform([predicted_class_idx])[0]
        else:  # Single output neuron
            predicted_class = int(np.round(raw_prediction[0][0]))
        
        # Map to class labels 
        class_labels = {
            1: "Class 1", 
            2: "Class 2", 
            3: "Class 3"
        }
        
        result = {
            'predicted_class': int(predicted_class),
            'class_label': class_labels.get(int(predicted_class), "Unknown")
        }
        
        return jsonify(result)
    
    except Exception as e:
        return jsonify({'error': str(e)})

@app.route('/batch_predict', methods=['POST'])
def batch_predict():
    try:
        # Expecting a CSV file upload
        if 'file' not in request.files:
            return jsonify({'error': 'No file part'})
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'No selected file'})
        
        # Read the uploaded CSV
        df = pd.read_csv(file)
        
        # Get features from the dataframe
        # Make sure the CSV has the same columns as your training data
        feature_columns = [col for col in df.columns if col != 'target']
        
        # Apply scaling if needed
        features_scaled = scaler.transform(df[feature_columns])
        
        # Make predictions
        raw_predictions = model.predict(features_scaled)
        
        # Process predictions based on model output format
        if raw_predictions.shape[1] > 1:  # One-hot encoded output
            predicted_classes = np.argmax(raw_predictions, axis=1)
            predicted_classes = label_encoder.inverse_transform(predicted_classes)
        else:  # Single output neuron
            predicted_classes = np.round(raw_predictions).astype(int).flatten()
        
        # Create result dataframe
        results = pd.DataFrame({
            'predicted_class': predicted_classes
        })
        
        # Convert to JSON
        return results.to_json(orient='records')
    
    except Exception as e:
        return jsonify({'error': str(e)})

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)